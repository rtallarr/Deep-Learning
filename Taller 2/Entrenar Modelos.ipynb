{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yLm5XV1XdT02"
      },
      "source": [
        "# Base Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZlDkjFWudVMG"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Rescaling, GlobalAveragePooling2D, Dense"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zvtuvW2v9V7b"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSwRxjOQ9Xm-",
        "outputId": "e5688615-3104-478b-dd25-1e82fb5b6ca8"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "RES_ESPACIAL = 32\n",
        "CHANNELS = 3\n",
        "NUM_CLASSES = 46\n",
        "\n",
        "def load_data(path,batch_size,res=RES_ESPACIAL):\n",
        "  return tf.keras.utils.image_dataset_from_directory(\n",
        "    path,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    image_size=(res,res),\n",
        "    shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 185101 files belonging to 46 classes.\n",
            "Found 46276 files belonging to 46 classes.\n"
          ]
        }
      ],
      "source": [
        "train_images = load_data(\"train_images_cropped/\",batch_size=BATCH_SIZE, res=RES_ESPACIAL)\n",
        "val_images = load_data(\"val_images_cropped/\",batch_size=BATCH_SIZE, res=RES_ESPACIAL)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zxu3180k_oZI"
      },
      "source": [
        "# Load Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4QfmlQBC_qA0"
      },
      "outputs": [],
      "source": [
        "train_metadata = pd.read_csv(\"metadata_train_df.csv\")\n",
        "val_metadata = pd.read_csv(\"metadata_val_df.csv\")\n",
        "test_metadata = pd.read_csv(\"metadata_test_df.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model - Categorias imagenes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenar modelo nuevo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_raw = keras.Input(shape=(RES_ESPACIAL,RES_ESPACIAL,CHANNELS))\n",
        "image_scaled = Rescaling(scale=1/255.0)(image_raw)\n",
        "visual = Conv2D(32,(3,3),activation='relu',padding='same')(image_scaled)\n",
        "visual = Conv2D(32,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(32,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(32,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(32,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(32,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = MaxPooling2D(pool_size=(2,2))(visual)\n",
        "visual = Conv2D(64,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(64,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(64,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(64,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = MaxPooling2D(pool_size=(2,2))(visual)\n",
        "visual = Conv2D(64,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(64,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = MaxPooling2D(pool_size=(2,2))(visual)\n",
        "visual = Conv2D(128,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(128,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(128,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(128,(3,3),activation='relu',padding='same')(visual)\n",
        "features = GlobalAveragePooling2D()(visual)\n",
        "hidden =  Dense(256, activation='relu')(features)\n",
        "outputs = Dense(NUM_CLASSES, activation='softmax')(hidden)\n",
        "\n",
        "model = tf.keras.Model(inputs=image_raw, outputs=outputs) \n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 128)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 46)                11822     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 811,726\n",
            "Trainable params: 811,726\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 10 mins aprox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1447/1447 [==============================] - 1169s 807ms/step - loss: 2.4579 - accuracy: 0.3038 - val_loss: 2.1746 - val_accuracy: 0.3692\n",
            "Epoch 2/10\n",
            "1447/1447 [==============================] - 1116s 770ms/step - loss: 2.0256 - accuracy: 0.4041 - val_loss: 1.9411 - val_accuracy: 0.4243\n",
            "Epoch 3/10\n",
            "1447/1447 [==============================] - 1113s 768ms/step - loss: 1.8474 - accuracy: 0.4483 - val_loss: 1.7929 - val_accuracy: 0.4643\n",
            "Epoch 4/10\n",
            "1447/1447 [==============================] - 1114s 769ms/step - loss: 1.7530 - accuracy: 0.4748 - val_loss: 1.7546 - val_accuracy: 0.4745\n",
            "Epoch 5/10\n",
            "1447/1447 [==============================] - 1116s 770ms/step - loss: 1.6928 - accuracy: 0.4927 - val_loss: 1.7094 - val_accuracy: 0.4863\n",
            "Epoch 6/10\n",
            "1447/1447 [==============================] - 1114s 769ms/step - loss: 1.6470 - accuracy: 0.5053 - val_loss: 1.6804 - val_accuracy: 0.4968\n",
            "Epoch 7/10\n",
            "1447/1447 [==============================] - 1110s 766ms/step - loss: 1.6068 - accuracy: 0.5168 - val_loss: 1.6843 - val_accuracy: 0.4953\n",
            "Epoch 8/10\n",
            "1447/1447 [==============================] - 1116s 770ms/step - loss: 1.5724 - accuracy: 0.5276 - val_loss: 1.6354 - val_accuracy: 0.5156\n",
            "Epoch 9/10\n",
            "1447/1447 [==============================] - 1118s 771ms/step - loss: 1.5426 - accuracy: 0.5353 - val_loss: 1.6260 - val_accuracy: 0.5142\n",
            "Epoch 10/10\n",
            "1447/1447 [==============================] - 1118s 771ms/step - loss: 1.5146 - accuracy: 0.5434 - val_loss: 1.6377 - val_accuracy: 0.5186\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "optimizer = Adam()\n",
        "model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
        "history_combinado = model.fit(train_images, epochs=NUM_EPOCHS, validation_data=val_images)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardar modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "model_guardar = tf.keras.Model(inputs=image_raw, outputs=outputs)\n",
        "#model_guardar.summary()\n",
        "model_guardar.save('Modelos/Categorias 3H sin corte.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usar modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "model_entrenado = tf.keras.models.load_model('Modelos/Modelo Categorias.h5')\n",
        "model_entrenado.trainable = False\n",
        "\n",
        "hidden = tf.keras.layers.Dense(64, activation='relu')(model_entrenado.output)\n",
        "output = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(hidden)\n",
        "model = tf.keras.Model(inputs=model_entrenado.input, outputs=output)\n",
        "#model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model - Atributos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-procesar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape de atributos de entrenamiento:  (37020, 1000)\n",
            "shape de atributos de validacion:  (46276, 1000)\n"
          ]
        }
      ],
      "source": [
        "def extract_metadata(filename):\n",
        "    attributes_list = []\n",
        "    corte_list = []\n",
        "    f = pd.read_csv(filename)\n",
        "    for _, row in f.iterrows():\n",
        "        line = row['attribute_labels']\n",
        "        list_aux = line.split(\" \")\n",
        "        list_aux = list(filter(None, list_aux))\n",
        "        list_aux = list(map(int, list_aux))\n",
        "        attributes_list.append(np.array(list_aux))\n",
        "\n",
        "        x1 = row['x_1']\n",
        "        y1 = row['y_1']\n",
        "        x2 = row['x_2']\n",
        "        y2 = row['y_2']\n",
        "        corte_list.append(np.array([x1,y1,x2,y2]))\n",
        "    return np.array(attributes_list), np.array(corte_list)\n",
        "\n",
        "train_att_list , train_cuadrados_list = extract_metadata(\"metadata_reduced_train_df_sorted.csv\")\n",
        "val_att_list , val_cuadrados_list = extract_metadata(\"metadata_val_df_sorted.csv\")\n",
        "\n",
        "print(\"shape de atributos de entrenamiento: \", train_att_list.shape)\n",
        "print(\"shape de atributos de validacion: \", val_att_list.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Se han extraido imagenes de reduced_train_images_no_class/ con exito\n",
            "El dataset es de dimensiones: (100, 64, 64, 3)\n",
            "Imagenes no cortadas:  0\n",
            "Se han extraido imagenes de val_images_no_class/ con exito\n",
            "El dataset es de dimensiones: (100, 64, 64, 3)\n",
            "Imagenes no cortadas:  0\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 128\n",
        "RES_ESPACIAL = 64\n",
        "CHANNELS = 3\n",
        "NUM_CLASSES = 1000\n",
        "\n",
        "train_images_folder = 'reduced_train_images_no_class/'\n",
        "val_images_folder = 'val_images_no_class/'\n",
        "\n",
        "def create_dataset(images_folder, cuadrados_list, sample_size=100):\n",
        "    images = os.listdir(images_folder)\n",
        "    sorted_images = sorted(images, key=lambda x: int(x.split(\".\")[0].split(\"/\")[-1].split(\"train_item\")[1]))\n",
        "\n",
        "    image_list = []\n",
        "    i = 0\n",
        "    nonCroppedImg = 0\n",
        "\n",
        "    for image in sorted_images[:sample_size]:\n",
        "        cuadrado = cuadrados_list[i]\n",
        "        image_path = images_folder+image\n",
        "        #print(image_path)\n",
        "        image = tf.keras.utils.load_img(image_path)\n",
        "        input_arr = tf.keras.utils.img_to_array(image)\n",
        "        #print(input_arr.shape, cuadrado[1],cuadrado[3],cuadrado[0],cuadrado[2])\n",
        "        cropped_img = input_arr[cuadrado[1]:cuadrado[3], cuadrado[0]:cuadrado[2]]\n",
        "        #print(cropped_img.shape, cuadrado[1],cuadrado[3],cuadrado[0],cuadrado[2])\n",
        "        cropped_img = np.array([cropped_img])  # Convert single image to a batch.\n",
        "        if cropped_img.shape[1] == 0 or cropped_img.shape[2] == 0:\n",
        "            resized_image = np.array(tf.image.resize(np.array([input_arr]), [RES_ESPACIAL, RES_ESPACIAL])[0])\n",
        "            image_list.append(resized_image)\n",
        "            nonCroppedImg += 1\n",
        "        else:\n",
        "            resized_image = np.array(tf.image.resize(cropped_img, [RES_ESPACIAL, RES_ESPACIAL])[0])\n",
        "            image_list.append(resized_image)\n",
        "        i += 1\n",
        "    print(\"Se han extraido imagenes de\", images_folder, \"con exito\")\n",
        "    print(\"El dataset es de dimensiones:\", np.array(image_list).shape)\n",
        "    print(\"Imagenes no cortadas: \", nonCroppedImg)\n",
        "\n",
        "    return np.array(image_list)\n",
        "\n",
        "train_images = create_dataset(train_images_folder, train_cuadrados_list)\n",
        "val_images = create_dataset(val_images_folder, val_cuadrados_list)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenar modelo nuevo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_raw = keras.Input(shape=(RES_ESPACIAL,RES_ESPACIAL,CHANNELS))\n",
        "image_scaled = Rescaling(scale=1/255.0)(image_raw)\n",
        "visual = Conv2D(32,(3,3),activation='relu',padding='same')(image_scaled)\n",
        "visual = Conv2D(32,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = MaxPooling2D(pool_size=(2,2))(visual)\n",
        "visual = Conv2D(64,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = Conv2D(64,(3,3),activation='relu',padding='same')(visual)\n",
        "visual = MaxPooling2D(pool_size=(2,2))(visual)\n",
        "visual = Conv2D(128,(3,3),activation='relu',padding='same')(visual)\n",
        "features = GlobalAveragePooling2D()(visual)\n",
        "hidden =  Dense(256, activation='relu')(features)\n",
        "outputs = Dense(NUM_CLASSES, activation='sigmoid')(hidden)\n",
        "\n",
        "model = tf.keras.Model(inputs=image_raw, outputs=outputs) \n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 31s 193ms/step - loss: 0.0194 - accuracy: 0.0134\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 1\n",
        "\n",
        "optimizer = Adam()\n",
        "model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
        "history_atributos = model.fit(train_images, train_att_list[:len(train_images)], validation_data=[val_images, val_att_list[:len(val_images)]], epochs=NUM_EPOCHS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardar modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_guardar = tf.keras.Model(inputs=image_raw, outputs=outputs)\n",
        "model_guardar.summary()\n",
        "model_guardar.save('Modelo atributos.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usar modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
            "                                                                 \n",
            " rescaling_2 (Rescaling)     (None, 64, 64, 3)         0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 64, 64, 32)        896       \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 64, 64, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 32, 32, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 16, 16, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " global_average_pooling2d_2   (None, 128)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1000)              257000    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                64064     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1000)              65000     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 558,512\n",
            "Trainable params: 129,064\n",
            "Non-trainable params: 429,448\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_entrenado = tf.keras.models.load_model('MODELO_ATRIBUTOS_1EPOCH_10000SAMPLESIZE.h5')\n",
        "model_entrenado.trainable = False\n",
        "\n",
        "hidden = tf.keras.layers.Dense(64, activation='relu')(model_entrenado.output)\n",
        "output = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(hidden)\n",
        "model = tf.keras.Model(inputs=model_entrenado.input, outputs=output)\n",
        "#model.summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EkFGgLo7s8tE",
        "bsVED_Pw1WJO",
        "XzdMTP1z_Xhe"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
