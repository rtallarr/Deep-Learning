{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow_addons as tfa\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/'\n",
    "PATH_ODIO = 'Odio/'\n",
    "PATH_CATEGORIAS = 'Categorias/'\n",
    "N_MAX_WORDS = 5000\n",
    "MAX_TEXT_LENGTH = 25\n",
    "\n",
    "def clean_text(text):\n",
    "  TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "  TEXT_CLEANING_RE_EXTRA = \"[^\\w\\s]\"\n",
    "  TEXT_CLEAN_CHAR = \"(\\s\\w\\s)\"\n",
    "  text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "  text = re.sub(TEXT_CLEANING_RE_EXTRA, ' ', str(text).lower()).strip()\n",
    "  text = re.sub(TEXT_CLEAN_CHAR, ' ', str(text).lower()).strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(PATH+'public_test.csv')\n",
    "tweets = data_test['text'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing - Odio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"spanish\")\n",
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text,cleaning=True,stopwords=True,stemming=True):\n",
    "    if cleaning:\n",
    "      text = clean_text(text)\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if (not stopwords) or (stopwords and (token not in stop_words)):\n",
    "            if stemming:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [preprocess(tweet, stemming=False) for tweet in tweets]\n",
    "TF_tokenizer = Tokenizer(num_words=N_MAX_WORDS)\n",
    "TF_tokenizer.fit_on_texts(cleaned)\n",
    "test_vectors = TF_tokenizer.texts_to_matrix(cleaned, mode='tfidf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing - Categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'\n",
    "bert_layer = hub.KerasLayer(bert_path, trainable=True)\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "bert_tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_for_bert(texts, bert_tokenizer, max_seq_len):\n",
    "  x, y, z =[],[], []\n",
    "  for text in texts:\n",
    "    tokens = bert_tokenizer.tokenize(text)\n",
    "    tokens = tokens[:min(len(tokens),max_seq_len-2)]\n",
    "    tokens = [\"[CLS]\"]+tokens+[\"[SEP]\"]\n",
    "    token_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_len-len(token_ids))\n",
    "    input_mask = ([1] * len(token_ids)) + ([0] *  (max_seq_len - len(token_ids)))\n",
    "    input_type = ([0] * (len(token_ids)-1)) + [1] + ([0] *  (max_seq_len - len(token_ids)))\n",
    "    x.append(np.array(input_ids))\n",
    "    y.append(np.array(input_mask))\n",
    "    z.append(np.array(input_type))\n",
    "  return [tf.cast(np.array(x),tf.int32), tf.cast(np.array(y),tf.int32), tf.cast(np.array(z),tf.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_input_test = prepare_input_for_bert(tweets, bert_tokenizer, 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions - Odio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tfa.metrics.F1Score(name='F1',average='macro', num_classes=1)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_odio = tf.keras.models.load_model(PATH_ODIO+'Modelo_denso.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_odio.predict(test_vectors)\n",
    "for row in y_pred:\n",
    "    if row[0] > 0.5:\n",
    "        row[0] = 1\n",
    "    else:\n",
    "        row[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_pred, columns=['Odio'])\n",
    "y_pred['tweet_id'] = data_test['tweet_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_csv = y_pred[['tweet_id', 'Odio']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions - Categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_CATEGORIAS = [\n",
    "      tf.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tfa.metrics.F1Score(name='F1',average='macro', num_classes=4)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_categorias = tf.keras.models.load_model(PATH_CATEGORIAS+'BERT_based_model_F1-5930.h5', custom_objects={\"KerasLayer\": hub.KerasLayer}, compile=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 119s 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_categorias = model_categorias.predict(bert_input_test)\n",
    "for row in y_pred_categorias:\n",
    "    for i in range(4):\n",
    "        if row[i] > 0.5:\n",
    "            row[i] = 1\n",
    "        else:\n",
    "            row[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_categorias = pd.DataFrame(y_pred_categorias, columns=['Mujeres', 'Comunidad LGBTQ+', 'Comunidades Migrantes', 'Pueblos Originarios'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>Odio</th>\n",
       "      <th>Mujeres</th>\n",
       "      <th>Comunidad LGBTQ+</th>\n",
       "      <th>Comunidades Migrantes</th>\n",
       "      <th>Pueblos Originarios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1533854540763742209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1277756504519725057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1529500412402757632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167425893066838016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1399515878727749632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>1469006079782645762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>1528569883868508161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>1502776153001455616</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>1540938860363907073</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>1533854043403173926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2291 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id  Odio  Mujeres  Comunidad LGBTQ+  \\\n",
       "0     1533854540763742209   0.0      0.0               0.0   \n",
       "1     1277756504519725057   0.0      0.0               0.0   \n",
       "2     1529500412402757632   0.0      0.0               0.0   \n",
       "3     1167425893066838016   1.0      0.0               0.0   \n",
       "4     1399515878727749632   0.0      1.0               0.0   \n",
       "...                   ...   ...      ...               ...   \n",
       "2286  1469006079782645762   0.0      0.0               0.0   \n",
       "2287  1528569883868508161   0.0      0.0               0.0   \n",
       "2288  1502776153001455616   1.0      0.0               0.0   \n",
       "2289  1540938860363907073   1.0      0.0               1.0   \n",
       "2290  1533854043403173926   0.0      0.0               0.0   \n",
       "\n",
       "      Comunidades Migrantes  Pueblos Originarios  \n",
       "0                       0.0                  0.0  \n",
       "1                       0.0                  0.0  \n",
       "2                       0.0                  0.0  \n",
       "3                       0.0                  1.0  \n",
       "4                       0.0                  0.0  \n",
       "...                     ...                  ...  \n",
       "2286                    1.0                  0.0  \n",
       "2287                    0.0                  0.0  \n",
       "2288                    0.0                  0.0  \n",
       "2289                    0.0                  0.0  \n",
       "2290                    0.0                  0.0  \n",
       "\n",
       "[2291 rows x 6 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_csv = pd.concat([y_pred_csv, y_pred_categorias], axis=1)\n",
    "y_pred_csv.to_csv('predicciones.csv', index=False)\n",
    "y_pred_csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
